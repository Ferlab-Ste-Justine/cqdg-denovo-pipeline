plugins {
  id 'nf-schema@2.0.0'
}
//For more information about the parameters, run the pipeline with  --help:
  //nextflow run cqdg-denovo-pipeline/main.nf --help
params {
    help = null
	//Input-Output options
    sampleFile = null
    outputDir = null
    sampleFileFormat = "V1"
    sequencingType = "WGS"

	//References
    referenceGenome = null
    broad = null
    vepCache = null
    intervalsFile = null


	//Process-specific parameters
    TSfilterSNP = '99.0'
    TSfilterINDEL = '99.0'
    hardFilters = [[name: 'QD2', expression: 'QD < 2.0'],
		   [name: 'QD1', expression: 'QD < 1.0'],
                   [name: 'QUAL30', expression: 'QUAL < 30.0'],
                   [name: 'SOR3', expression: 'SOR > 3.0'],
                   [name: 'FS60', expression: 'FS > 60.0'],
                   [name: 'MQ40', expression: 'MQ < 40.0'],
                   [name: 'MQRankSum-12.5', expression: 'MQRankSum < -12.5'],
                   [name: 'ReadPosRankSum-8', expression: 'ReadPosRankSum < -8.0']]
	
	//Resources options
	//defaults expecting to be overwritten
	vepCpu = 4
	max_cpus = 16
	max_disk = 80.GB
	max_time = 12.h
	max_memory = 120.GB
}

process {
  disk = 40.GB
	withName: 'excludeMNPs|splitMultiAllelics' {
		container = 'staphb/bcftools:1.19'
	}
	withName: 'importGVCF|genotypeGVCF_multi|genotypeGVCF_solo|genotypeGVCF|variantRecalibratorSNP|variantRecalibratorIndel|applyVQSRIndel|applyVQSRSNP|hardFiltering|gatherVCF' {
		container = 'broadinstitute/gatk:4.5.0.0'
	}
	withName: vep {
		container = 'ensemblorg/ensembl-vep:release_111.0'
	}
	withName: tabix {
		container = 'staphb/htslib:1.19'
	}

	withName: 'excludeMNPs' {
		errorStrategy = 'retry'
		maxRetries = 1
		cpus	=	{ check_max( 2 		* task.attempt, 'cpus' 		) 	}
		memory	=	{ check_max( 16.GB 	* task.attempt, 'memory'	) 	}
		disk	=	{ check_max( 50.GB 	* task.attempt, 'disk' 		)	}
		time	=	{ check_max( 8.h  	* task.attempt, 'time' 		)	}
	}
	withName: 'importGVCF' {
		errorStrategy = 'retry'
		maxRetries = 1
		cpus	=	{ check_max( 4		* task.attempt, 'cpus'		) 	}
		memory	=	{ check_max( 16.GB	* task.attempt, 'memory'	) 	}
		disk	=	{ check_max( 80.GB	* task.attempt, 'disk'		)	}
		time	=	{ check_max( 8.h	* task.attempt, 'time'		)	}
	}
	withName: 'genotypeGVCF' {
		errorStrategy = 'retry'
		maxRetries = 2
		cpus	=	{ check_max( 2		* task.attempt, 'cpus' 		) 	}
		memory	=	{ check_max( 14.GB	* task.attempt, 'memory' 	) 	}
		disk	=	{ check_max( 40.GB	* task.attempt, 'disk' 		)	}
		time	=	{ check_max( 8.h	* task.attempt, 'time' 		)	}
	}
	withName: 'variantRecalibrator.*|apply.*' {
		errorStrategy = 'retry'
		maxRetries = 2
		cpus	=	{ check_max( 2		* task.attempt, 'cpus'		) 	}
		memory	=	{ check_max( 14.GB	* task.attempt, 'memory'	)	}
		disk	=	{ check_max( 30.GB	* task.attempt, 'disk'		)	}
		time	=	{ check_max( 10.h	* task.attempt, 'time'		)	}
	}
	withName: 'splitMultiAllelics' {
		errorStrategy = 'retry'
		maxRetries = 2
		cpus	=	{ check_max( 2		* task.attempt, 'cpus'		) 	}
		memory	=	{ check_max( 14.GB	* task.attempt, 'memory'	)	}
		disk	=	{ check_max( 30.GB	* task.attempt, 'disk'		)	}
		time	=	{ check_max( 10.h	* task.attempt, 'time'		)	}
	}
  	withName: 'hardFiltering' {
		errorStrategy = 'retry'
		maxRetries = 2
		cpus	=	{ check_max( 2		* task.attempt, 'cpus'		) 	}
		memory	=	{ check_max( 14.GB	* task.attempt, 'memory'	)	}
		disk	=	{ check_max( 30.GB	* task.attempt, 'disk'		)	}
		time	=	{ check_max( 10.h	* task.attempt, 'time'		)	}
	}
	withName: 'tabix' {
		errorStrategy = 'retry'
		maxRetries = 2
		cpus	=	{ check_max( 2		* task.attempt, 'cpus'		) 	}
		memory	=	{ check_max( 14.GB	* task.attempt, 'memory'	)	}
		disk	=	{ check_max( 30.GB	* task.attempt, 'disk'		)	}
		time	=	{ check_max( 10.h	* task.attempt, 'time'		)	}
	}
	withName: 'vep' {
		errorStrategy = 'retry'
		maxRetries = 2
		cpus	=	{ check_max( 4 		* task.attempt, 'cpus'		)	}
		memory	=	{ check_max( 16.GB	* task.attempt, 'memory'	)	}
		disk	=	{ check_max( 80.GB	* task.attempt, 'disk'		)	}
		time	=	{ check_max( 10.h 	* task.attempt, 'time'		)	}
	}
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
	else if (type == 'disk') {
        try {
            if (obj.compareTo(params.max_disk as nextflow.util.MemoryUnit) == 1)
                return params.max_disk as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max disk '${params.max_disk}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
executor {
  queueSize = 80
  pollInterval = '60 sec'
  submitRateLimit = '10/1min'
}

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.outputDir}/pipeline_info/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.outputDir}/pipeline_info/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.outputDir}/pipeline_info/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.outputDir}/pipeline_info/pipeline_dag_${trace_timestamp}.html"
}
